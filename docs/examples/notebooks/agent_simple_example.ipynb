{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.core import Settings\n",
    "import nest_asyncio\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama\n",
    "Settings.llm = Ollama(model=\"qwen2:0.5b\", request_timeout=360.0)\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the LlamaDebugHandler to print the trace of the sub questions\n",
    "# captured by the SUB_QUESTION callback event type\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "Settings.callback_manager = callback_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_embedding -> 13.896819 seconds\n",
      "    |_embedding -> 13.897055 seconds\n",
      "    |_embedding -> 13.897106 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "pg_essay = SimpleDirectoryReader(input_dir=\"./data\").load_data()\n",
    "\n",
    "# build index and query engine\n",
    "vector_query_engine = VectorStoreIndex.from_documents(\n",
    "    pg_essay,\n",
    "    use_async=True,\n",
    ").as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup base query engine as tool\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=vector_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"pg_essay\",\n",
    "            description=\"Paul Graham essay on What I Worked On\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[pg_essay] Q: Before YC\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[pg_essay] Q: During YC\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[pg_essay] Q: After YC\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[pg_essay] A: After YC.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[pg_essay] A: Before YC\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[pg_essay] A: Based on the context provided in the document, during YC (the startup consulting firm) there were two main projects that were working at the same time:\n",
      "\n",
      "1. \"YC was different from other kinds of work I've done.\" This suggests that YC was focusing on a very specific type of consulting and development work.\n",
      "\n",
      "2. There was also a \"new version of Arc\" that was being developed by Robert Morris. This indicates that Robert is actively working on new or revised versions of the software they are working on.\n",
      "\n",
      "So in summary, during YC there were two main projects: a consulting firm focused on developing new software, and a development team developing an update to the existing software.\n",
      "\u001b[0m**********\n",
      "Trace: query\n",
      "    |_query -> 136.049625 seconds\n",
      "      |_templating -> 3e-05 seconds\n",
      "      |_llm -> 2.424401 seconds\n",
      "      |_sub_question -> 104.016951 seconds\n",
      "        |_query -> 104.016448 seconds\n",
      "          |_retrieve -> 0.047654 seconds\n",
      "            |_embedding -> 0.044862 seconds\n",
      "          |_synthesize -> 103.96811 seconds\n",
      "            |_templating -> 2.8e-05 seconds\n",
      "            |_llm -> 103.956004 seconds\n",
      "      |_sub_question -> 109.848889 seconds\n",
      "        |_query -> 109.848305 seconds\n",
      "          |_retrieve -> 0.044136 seconds\n",
      "            |_embedding -> 0.041745 seconds\n",
      "          |_synthesize -> 109.80365 seconds\n",
      "            |_templating -> 2.6e-05 seconds\n",
      "            |_llm -> 109.79152 seconds\n",
      "      |_sub_question -> 103.870417 seconds\n",
      "        |_query -> 103.869888 seconds\n",
      "          |_retrieve -> 0.044442 seconds\n",
      "            |_embedding -> 0.042296 seconds\n",
      "          |_synthesize -> 103.824844 seconds\n",
      "            |_templating -> 2.7e-05 seconds\n",
      "            |_llm -> 103.812756 seconds\n",
      "      |_synthesize -> 23.687117 seconds\n",
      "        |_templating -> 3e-05 seconds\n",
      "        |_llm -> 23.682915 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How was Paul Grahams life different before, during, and after YC?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before YC:\n",
      "\n",
      "Paul Graham is a well-known American computer scientist who founded the startup consulting firm called Y Combinator. He was born on December 26, 1970.\n",
      "\n",
      "During his time at Y C, Paul worked with several notable figures in the tech industry such as Tim Berners-Lee, Elon Musk, Larry Page and Sergey Brin.\n",
      "\n",
      "After YC:\n",
      "\n",
      "Paul's tenure at Y C lasted from 2004 to 2008. He was responsible for shaping the direction of the firm. \n",
      "\n",
      "During this time he worked with several more notable figures in the tech industry like Steve Jobs, Jeff Bezos, and Larry Page. In addition, he helped bring about the creation of a new venture called \"Blue Brain\" which would later become known as Google.\n",
      "\n",
      "Paul's work at Y C had a significant impact on the tech industry and played a crucial role in its development. His leadership style and emphasis on innovation were instrumental in the firm's success.\n",
      "\n",
      "After YC:\n",
      "\n",
      "In his time at Y C, Paul continued to be involved with many more prominent figures in the tech industry such as Steve Jobs, Larry Page, Elon Musk, and Jeff Bezos. He also helped bring about the creation of a new venture called \"Blue Brain\" which would later become known as Google.\n",
      "\n",
      "So in summary, before YC he was a well-known computer scientist who founded Y Combinator and worked with several notable figures such as Tim Berners-Lee, Elon Musk, Larry Page, and Jeff Bezos. During this time he helped shape the direction of the firm and played a significant role in its development.\n",
      "\n",
      "After YC:\n",
      "\n",
      "Paul's tenure at Y C lasted from 2004 to 2008 when he worked with several more notable figures in the tech industry such as Steve Jobs, Jeff Bezos, and Larry Page. In addition, he helped bring about the creation of a new venture called \"Blue Brain\" which would later become known as Google.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub Question 0: Before YC\n",
      "Answer: Before YC\n",
      "====================================\n",
      "Sub Question 1: During YC\n",
      "Answer: Based on the context provided in the document, during YC (the startup consulting firm) there were two main projects that were working at the same time:\n",
      "\n",
      "1. \"YC was different from other kinds of work I've done.\" This suggests that YC was focusing on a very specific type of consulting and development work.\n",
      "\n",
      "2. There was also a \"new version of Arc\" that was being developed by Robert Morris. This indicates that Robert is actively working on new or revised versions of the software they are working on.\n",
      "\n",
      "So in summary, during YC there were two main projects: a consulting firm focused on developing new software, and a development team developing an update to the existing software.\n",
      "====================================\n",
      "Sub Question 2: After YC\n",
      "Answer: After YC.\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# iterate through sub_question items captured in SUB_QUESTION event\n",
    "from llama_index.core.callbacks import CBEventType, EventPayload\n",
    "\n",
    "for i, (start_event, end_event) in enumerate(\n",
    "    llama_debug.get_event_pairs(CBEventType.SUB_QUESTION)\n",
    "):\n",
    "    qa_pair = end_event.payload[EventPayload.SUB_QUESTION]\n",
    "    print(\"Sub Question \" + str(i) + \": \" + qa_pair.sub_q.sub_question.strip())\n",
    "    print(\"Answer: \" + qa_pair.answer.strip())\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
